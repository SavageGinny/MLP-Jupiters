{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPpBi48efnLVKfHV3vY0SNP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SavageGinny/MLP-Jupiters/blob/main/Lab1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Загружаем библиотеки"
      ],
      "metadata": {
        "id": "nX9tSP5RptDE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88gnbYCmpmWK"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import pymorphy2\n",
        "import pymorphy3\n",
        "from natasha import Doc, Segmenter, MorphVocab, NewsEmbedding, NewsMorphTagger\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Загрузка необходимых ресурсов nltk\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Загрузка необходимых ресурсов nltk\n"
      ],
      "metadata": {
        "id": "tkAYx7VQpwW1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "0_eT6IKAp0Yv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Текст"
      ],
      "metadata": {
        "id": "2YQ9zbo1qJ8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \"Пример обработки текста в Python :)\""
      ],
      "metadata": {
        "id": "28rDlLBeqLRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Лемантизация и стеминг на разных библиотеках"
      ],
      "metadata": {
        "id": "Y2C07Uygp6aL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize_and_stem(text):\n",
        "    morph_p2 = pymorphy2.MorphAnalyzer()\n",
        "    morph_p3 = pymorphy3.MorphAnalyzer()\n",
        "    segmenter = Segmenter()\n",
        "    emb = NewsEmbedding()\n",
        "    morph_tagger = NewsMorphTagger(emb)\n",
        "    morph_vocab = MorphVocab()\n",
        "\n",
        "    stemmer = SnowballStemmer(\"russian\")\n",
        "\n",
        "    words = word_tokenize(text, language='russian')\n",
        "\n",
        "    results = {\n",
        "        \"original\": words,\n",
        "        \"pymorphy2\": [morph_p2.parse(word)[0].normal_form for word in words],\n",
        "        \"pymorphy3\": [morph_p3.parse(word)[0].normal_form for word in words],\n",
        "        \"natasha\": []\n",
        "    }\n",
        "\n",
        "    doc = Doc(text)\n",
        "    doc.segment(segmenter)\n",
        "    doc.tag_morph(morph_tagger)\n",
        "    for token in doc.tokens:\n",
        "        token.lemmatize(morph_vocab)\n",
        "        results[\"natasha\"].append(token.lemma)\n",
        "\n",
        "    results[\"stemmed\"] = [stemmer.stem(word) for word in words]\n",
        "\n",
        "    return results\n",
        "\n",
        "lemmatized_stemmed = lemmatize_and_stem(sample_text)"
      ],
      "metadata": {
        "id": "_pYp2Wiup3t-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Смотрим, что получилось"
      ],
      "metadata": {
        "id": "ng87CLkrqYge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatized_stemmed"
      ],
      "metadata": {
        "id": "0xrtMSEBqbui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Токенизатор"
      ],
      "metadata": {
        "id": "g0B5lurdp--d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_ascii_rus(text):\n",
        "    return [char for char in text if ord(char)>1000]"
      ],
      "metadata": {
        "id": "6OsDE7SZqCKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenize_ascii_rus(\" \".join(lemmatized_stemmed[\"original\"]))"
      ],
      "metadata": {
        "id": "KR7vHskKqn7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Векторизатор"
      ],
      "metadata": {
        "id": "GIXcH39LqC9d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize_ascii_rus(text):\n",
        "    return np.array([ord(char) for char in text if ord(char)>1000])"
      ],
      "metadata": {
        "id": "cNYnHO8nqEWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorized = vectorize_ascii_rus(\" \".join(lemmatized_stemmed[\"original\"]))"
      ],
      "metadata": {
        "id": "o2_MQm2yqwQF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}