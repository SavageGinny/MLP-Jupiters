{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPq8tifhbiHJKh2ivfxr/Nx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SavageGinny/MLP-Jupiters/blob/main/lab4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Загружаем библиотеки"
      ],
      "metadata": {
        "id": "nX9tSP5RptDE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "88gnbYCmpmWK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import random\n",
        "import math\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Обработчик текста**"
      ],
      "metadata": {
        "id": "2YQ9zbo1qJ8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TextProcessor:\n",
        "    def __init__(self):\n",
        "        self.vocab = {}\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = {}\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        text = text.lower()\n",
        "        tokens = re.findall(r'\\b\\w+\\b', text, re.UNICODE)\n",
        "        return tokens\n",
        "\n",
        "    def lemmatize(self, tokens):\n",
        "        # Простая псевдо-лемматизация (можно заменить словарем)\n",
        "        lemmas = [re.sub(r'(ами|ами|ами|ов|ев|ий|ый|ой|ого|ому|ым|ах|ях|е|у|ю|а|о|ы|и|я)$', '', word) for word in tokens]\n",
        "        return lemmas\n",
        "\n",
        "    def build_vocab(self, tokens):\n",
        "        self.vocab = Counter(tokens)\n",
        "        self.word2idx = {word: i for i, word in enumerate(self.vocab)}\n",
        "        self.idx2word = {i: word for word, i in self.word2idx.items()}\n",
        "\n",
        "    def encode(self, tokens):\n",
        "        return [self.word2idx[word] for word in tokens if word in self.word2idx]\n",
        "\n",
        "    def decode(self, indices):\n",
        "        return [self.idx2word[i] for i in indices]\n",
        "\n"
      ],
      "metadata": {
        "id": "28rDlLBeqLRh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GPT нейросеть**"
      ],
      "metadata": {
        "id": "m-s0VhiZ538i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleGPT:\n",
        "    def __init__(self, vocab_size, hidden_dim):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.W1 = np.random.randn(vocab_size, hidden_dim) * 0.01\n",
        "        self.W2 = np.random.randn(hidden_dim, vocab_size) * 0.01\n",
        "\n",
        "    def softmax(self, x):\n",
        "        e_x = np.exp(x - np.max(x))\n",
        "        return e_x / e_x.sum(axis=0)\n",
        "\n",
        "    def train(self, X, y, epochs=100, lr=0.01):\n",
        "        for epoch in range(epochs):\n",
        "            loss = 0\n",
        "            for context, target in zip(X, y):\n",
        "                x_vec = np.sum(self.W1[context], axis=0)\n",
        "                h = x_vec / len(context)\n",
        "                u = np.dot(h, self.W2)\n",
        "                y_pred = self.softmax(u)\n",
        "\n",
        "                loss += -np.log(y_pred[target])\n",
        "\n",
        "                # Градиенты\n",
        "                e = y_pred\n",
        "                e[target] -= 1\n",
        "\n",
        "                dW2 = np.outer(h, e)\n",
        "                dW1 = np.zeros_like(self.W1)\n",
        "                for idx in context:\n",
        "                    dW1[idx] += np.dot(self.W2, e) / len(context)\n",
        "\n",
        "                # Обновление весов\n",
        "                self.W1 -= lr * dW1\n",
        "                self.W2 -= lr * dW2\n",
        "\n",
        "            if epoch % 10 == 0:\n",
        "                print(f\"Epoch {epoch}, Loss: {loss}\")\n",
        "\n",
        "    def predict(self, context):\n",
        "        x_vec = np.sum(self.W1[context], axis=0)\n",
        "        h = x_vec / len(context)\n",
        "        u = np.dot(h, self.W2)\n",
        "        y_pred = self.softmax(u)\n",
        "        return np.argmax(y_pred)\n",
        "\n",
        "    def generate(self, start_context, n_words=10):\n",
        "        generated = start_context[:]\n",
        "        for _ in range(n_words):\n",
        "            idxs = self.encode(generated[-2:])  # bigram context\n",
        "            if len(idxs) < 2:\n",
        "                break\n",
        "            next_idx = self.predict(idxs)\n",
        "            generated.append(self.idx2word[next_idx])\n",
        "        return ' '.join(generated)\n",
        "\n",
        "    def encode(self, tokens):\n",
        "        return [self.word2idx.get(t, 0) for t in tokens]"
      ],
      "metadata": {
        "id": "OXzvZEGR4jGZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Загрузка датасета**"
      ],
      "metadata": {
        "id": "m7NGHlcM5V_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"naruto_full_dataset.txt\", encoding='utf-8') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "k5EIOFhrgIPR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Подготовка данных**\n"
      ],
      "metadata": {
        "id": "g0B5lurdp--d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tp = TextProcessor()\n",
        "tokens = tp.tokenize(text)\n",
        "lemmas = tp.lemmatize(tokens)\n",
        "tp.build_vocab(lemmas)\n",
        "\n",
        "window_size = 2\n",
        "data = []\n",
        "for i in range(window_size, len(lemmas) - window_size):\n",
        "    context = lemmas[i - window_size:i] + lemmas[i + 1:i + window_size + 1]\n",
        "    target = lemmas[i]\n",
        "    data.append((context, target))\n",
        "\n",
        "\n",
        "X = [tp.encode(context) for context, target in data]\n",
        "y = [tp.word2idx[target] for context, target in data]"
      ],
      "metadata": {
        "id": "6OsDE7SZqCKT"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Обучение модели**"
      ],
      "metadata": {
        "id": "GIXcH39LqC9d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpt = SimpleGPT(vocab_size=len(tp.vocab), hidden_dim=50)\n",
        "gpt.word2idx = tp.word2idx\n",
        "gpt.idx2word = tp.idx2word\n",
        "gpt.train(X, y, epochs=500, lr=0.1)"
      ],
      "metadata": {
        "id": "cNYnHO8nqEWP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f13b4dda-f80b-435d-c7d8-7c4056de6fd2"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 11108.092303630261\n",
            "Epoch 10, Loss: 2162.191874176721\n",
            "Epoch 20, Loss: 125.99707077306324\n",
            "Epoch 30, Loss: 46.20133178103384\n",
            "Epoch 40, Loss: 26.985535773563473\n",
            "Epoch 50, Loss: 18.707313462763402\n",
            "Epoch 60, Loss: 14.172267630043033\n",
            "Epoch 70, Loss: 11.335368763444858\n",
            "Epoch 80, Loss: 9.404287857531752\n",
            "Epoch 90, Loss: 8.01042730232998\n",
            "Epoch 100, Loss: 6.9600241524079705\n",
            "Epoch 110, Loss: 6.141864463320907\n",
            "Epoch 120, Loss: 5.487729684960028\n",
            "Epoch 130, Loss: 4.953537829603535\n",
            "Epoch 140, Loss: 4.509583149350543\n",
            "Epoch 150, Loss: 4.13514406141754\n",
            "Epoch 160, Loss: 3.8153413682507926\n",
            "Epoch 170, Loss: 3.539224664673514\n",
            "Epoch 180, Loss: 3.298562257018554\n",
            "Epoch 190, Loss: 3.087050938519558\n",
            "Epoch 200, Loss: 2.8997853420542588\n",
            "Epoch 210, Loss: 2.7328927851962557\n",
            "Epoch 220, Loss: 2.583276510616129\n",
            "Epoch 230, Loss: 2.4484316415781566\n",
            "Epoch 240, Loss: 2.3263109683301444\n",
            "Epoch 250, Loss: 2.215225543038686\n",
            "Epoch 260, Loss: 2.1137700135125868\n",
            "Epoch 270, Loss: 2.0207658169413554\n",
            "Epoch 280, Loss: 1.9352174532233282\n",
            "Epoch 290, Loss: 1.8562784631825058\n",
            "Epoch 300, Loss: 1.7832246948175352\n",
            "Epoch 310, Loss: 1.7154331036483577\n",
            "Epoch 320, Loss: 1.652364798666602\n",
            "Epoch 330, Loss: 1.5935513765508325\n",
            "Epoch 340, Loss: 1.5385838253363169\n",
            "Epoch 350, Loss: 1.4871034525191027\n",
            "Epoch 360, Loss: 1.43879442054981\n",
            "Epoch 370, Loss: 1.393377567859203\n",
            "Epoch 380, Loss: 1.3506052650175069\n",
            "Epoch 390, Loss: 1.3102571097499207\n",
            "Epoch 400, Loss: 1.2721363058582595\n",
            "Epoch 410, Loss: 1.2360666029023843\n",
            "Epoch 420, Loss: 1.201889698150735\n",
            "Epoch 430, Loss: 1.1694630215553734\n",
            "Epoch 440, Loss: 1.1386578396292706\n",
            "Epoch 450, Loss: 1.1093576260614182\n",
            "Epoch 460, Loss: 1.0814566564129795\n",
            "Epoch 470, Loss: 1.0548587918459658\n",
            "Epoch 480, Loss: 1.0294764229491695\n",
            "Epoch 490, Loss: 1.0052295496711179\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Запуск**"
      ],
      "metadata": {
        "id": "8qh-JxLXj36u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_input = input(\"Введите начальные 1-2 слова: \").strip().lower()\n",
        "start_tokens = tp.tokenize(user_input)\n",
        "start_lemmas = tp.lemmatize(start_tokens)\n",
        "start_known = [lemma for lemma in start_lemmas if lemma in tp.word2idx]\n",
        "\n",
        "if len(start_known) < 2:\n",
        "    print(\"Недостаточно известных слов в вводе. Пожалуйста, введите минимум два известных слова.\")\n",
        "else:\n",
        "    print(\"Сгенерированный текст:\")\n",
        "    print(gpt.generate(start_known, n_words=15))"
      ],
      "metadata": {
        "id": "VhoysY5H5qaF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "289bc375-13d1-4322-b658-4d3962ccab74"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Введите начальные 1-2 слова: Чакра — это энергия\n",
            "Сгенерированный текст:\n",
            "чакр эт энерги чакр природ тр чакр природ тр чакр природ тр чакр природ тр чакр природ тр\n"
          ]
        }
      ]
    }
  ]
}