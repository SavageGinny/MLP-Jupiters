{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPN316grlYNBbeTybvAYtyb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SavageGinny/MLP-Jupiters/blob/main/Lab1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymorphy2 pymorphy3 natasha"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lu2N56Q1rLD9",
        "outputId": "1d2b8db4-d8b0-4fe4-9c30-06f785c5add6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymorphy2\n",
            "  Downloading pymorphy2-0.9.1-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting pymorphy3\n",
            "  Downloading pymorphy3-2.0.3-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting natasha\n",
            "  Downloading natasha-1.6.0-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting dawg-python>=0.7.1 (from pymorphy2)\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting pymorphy2-dicts-ru<3.0,>=2.4 (from pymorphy2)\n",
            "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting docopt>=0.6 (from pymorphy2)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting dawg2-python>=0.8.0 (from pymorphy3)\n",
            "  Downloading dawg2_python-0.9.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting pymorphy3-dicts-ru (from pymorphy3)\n",
            "  Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting razdel>=0.5.0 (from natasha)\n",
            "  Downloading razdel-0.5.0-py3-none-any.whl.metadata (10.0 kB)\n",
            "Collecting navec>=0.9.0 (from natasha)\n",
            "  Downloading navec-0.10.0-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting slovnet>=0.6.0 (from natasha)\n",
            "  Downloading slovnet-0.6.0-py3-none-any.whl.metadata (34 kB)\n",
            "Collecting yargy>=0.16.0 (from natasha)\n",
            "  Downloading yargy-0.16.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting ipymarkup>=0.8.0 (from natasha)\n",
            "  Downloading ipymarkup-0.9.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting intervaltree>=3 (from ipymarkup>=0.8.0->natasha)\n",
            "  Downloading intervaltree-3.1.0.tar.gz (32 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from navec>=0.9.0->natasha) (1.26.4)\n",
            "Requirement already satisfied: sortedcontainers<3.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from intervaltree>=3->ipymarkup>=0.8.0->natasha) (2.4.0)\n",
            "Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pymorphy3-2.0.3-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading natasha-1.6.0-py3-none-any.whl (34.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Downloading dawg2_python-0.9.0-py3-none-any.whl (9.3 kB)\n",
            "Downloading ipymarkup-0.9.0-py3-none-any.whl (14 kB)\n",
            "Downloading navec-0.10.0-py3-none-any.whl (23 kB)\n",
            "Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m123.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading razdel-0.5.0-py3-none-any.whl (21 kB)\n",
            "Downloading slovnet-0.6.0-py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.7/46.7 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yargy-0.16.0-py3-none-any.whl (33 kB)\n",
            "Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m94.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: docopt, intervaltree\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=3ceee908c3ccd552c3197fe860f342d4a56ce13acd37a4e0724a3125dab1f988\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/b0/8c/4b75c4116c31f83c8f9f047231251e13cc74481cca4a78a9ce\n",
            "  Building wheel for intervaltree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for intervaltree: filename=intervaltree-3.1.0-py2.py3-none-any.whl size=26097 sha256=45eb9e49812ec67cc875b5d8d04475fc188a698d2c443744975d4af70e784aec\n",
            "  Stored in directory: /root/.cache/pip/wheels/31/d7/d9/eec6891f78cac19a693bd40ecb8365d2f4613318c145ec9816\n",
            "Successfully built docopt intervaltree\n",
            "Installing collected packages: razdel, pymorphy3-dicts-ru, pymorphy2-dicts-ru, docopt, dawg-python, pymorphy2, navec, intervaltree, dawg2-python, yargy, slovnet, pymorphy3, ipymarkup, natasha\n",
            "Successfully installed dawg-python-0.7.2 dawg2-python-0.9.0 docopt-0.6.2 intervaltree-3.1.0 ipymarkup-0.9.0 natasha-1.6.0 navec-0.10.0 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844 pymorphy3-2.0.3 pymorphy3-dicts-ru-2.4.417150.4580142 razdel-0.5.0 slovnet-0.6.0 yargy-0.16.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Загружаем библиотеки"
      ],
      "metadata": {
        "id": "nX9tSP5RptDE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88gnbYCmpmWK",
        "outputId": "27c81f11-51f2-4cee-93c7-9889f5c89e38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import pymorphy2\n",
        "import pymorphy3\n",
        "from natasha import Doc, Segmenter, MorphVocab, NewsEmbedding, NewsMorphTagger\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Загрузка необходимых ресурсов nltk\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Загрузка необходимых ресурсов nltk\n"
      ],
      "metadata": {
        "id": "tkAYx7VQpwW1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_eT6IKAp0Yv",
        "outputId": "d64ca68b-6a88-4cd1-9f3f-60c7052c931f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Текст"
      ],
      "metadata": {
        "id": "2YQ9zbo1qJ8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \"Пример обработки текста в Python :)\""
      ],
      "metadata": {
        "id": "28rDlLBeqLRh"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Лемантизация и стеминг на разных библиотеках"
      ],
      "metadata": {
        "id": "Y2C07Uygp6aL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize_and_stem(text):\n",
        "    morph_p2 = pymorphy2.MorphAnalyzer()\n",
        "    morph_p3 = pymorphy3.MorphAnalyzer()\n",
        "    segmenter = Segmenter()\n",
        "    emb = NewsEmbedding()\n",
        "    morph_tagger = NewsMorphTagger(emb)\n",
        "    morph_vocab = MorphVocab()\n",
        "\n",
        "    stemmer = SnowballStemmer(\"russian\")\n",
        "\n",
        "    words = word_tokenize(text, language='russian')\n",
        "\n",
        "    results = {\n",
        "        \"original\": words,\n",
        "        \"pymorphy2\": [morph_p2.parse(word)[0].normal_form for word in words],\n",
        "        \"pymorphy3\": [morph_p3.parse(word)[0].normal_form for word in words],\n",
        "        \"natasha\": []\n",
        "    }\n",
        "\n",
        "    doc = Doc(text)\n",
        "    doc.segment(segmenter)\n",
        "    doc.tag_morph(morph_tagger)\n",
        "    for token in doc.tokens:\n",
        "        token.lemmatize(morph_vocab)\n",
        "        results[\"natasha\"].append(token.lemma)\n",
        "\n",
        "    results[\"stemmed\"] = [stemmer.stem(word) for word in words]\n",
        "\n",
        "    return results\n",
        "\n",
        "lemmatized_stemmed = lemmatize_and_stem(sample_text)"
      ],
      "metadata": {
        "id": "_pYp2Wiup3t-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Смотрим, что получилось"
      ],
      "metadata": {
        "id": "ng87CLkrqYge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatized_stemmed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xrtMSEBqbui",
        "outputId": "6063fa75-58b4-436d-c961-f60f3a23b8c1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'original': ['Пример', 'обработки', 'текста', 'в', 'Python', ':', ')'],\n",
              " 'pymorphy2': ['пример', 'обработка', 'текст', 'в', 'python', ':', ')'],\n",
              " 'pymorphy3': ['пример', 'обработка', 'текст', 'в', 'python', ':', ')'],\n",
              " 'natasha': ['пример', 'обработка', 'текст', 'в', 'python', ':)'],\n",
              " 'stemmed': ['пример', 'обработк', 'текст', 'в', 'Python', ':', ')']}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Токенизатор"
      ],
      "metadata": {
        "id": "g0B5lurdp--d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_ascii_rus(text):\n",
        "    return [char for char in text if ord(char)>1000]"
      ],
      "metadata": {
        "id": "6OsDE7SZqCKT"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenize_ascii_rus(\" \".join(lemmatized_stemmed[\"original\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KR7vHskKqn7y",
        "outputId": "cae94d66-e680-4243-aef2-04e400741e15"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['П',\n",
              " 'р',\n",
              " 'и',\n",
              " 'м',\n",
              " 'е',\n",
              " 'р',\n",
              " 'о',\n",
              " 'б',\n",
              " 'р',\n",
              " 'а',\n",
              " 'б',\n",
              " 'о',\n",
              " 'т',\n",
              " 'к',\n",
              " 'и',\n",
              " 'т',\n",
              " 'е',\n",
              " 'к',\n",
              " 'с',\n",
              " 'т',\n",
              " 'а',\n",
              " 'в']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Векторизатор"
      ],
      "metadata": {
        "id": "GIXcH39LqC9d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize_ascii_rus(text):\n",
        "    return np.array([ord(char) for char in text if ord(char)>1000])"
      ],
      "metadata": {
        "id": "cNYnHO8nqEWP"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorize_ascii_rus(\" \".join(lemmatized_stemmed[\"original\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2_MQm2yqwQF",
        "outputId": "e5acea10-83bf-415f-f38d-5b4ed7999802"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1055, 1088, 1080, 1084, 1077, 1088, 1086, 1073, 1088, 1072, 1073,\n",
              "       1086, 1090, 1082, 1080, 1090, 1077, 1082, 1089, 1090, 1072, 1074])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    }
  ]
}